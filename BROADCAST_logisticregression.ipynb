{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VRConfig = {\n",
    "    'rounds': 10,\n",
    "    'displayInterval': 4000,\n",
    "    \n",
    "    'weight_decay': 0.01,\n",
    "    'honestSize': 50,\n",
    "    'byzantineSize': 20,\n",
    "    \n",
    "    'SEED': 200,\n",
    "    'fixSeed': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ijcnn1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataSetConfig = {\n",
    "#     'name': 'ijcnn1',\n",
    "#     'dataSet' : 'ijcnn1',\n",
    "#     'dataSetSize': 49990,\n",
    "#     'maxFeature': 22,\n",
    "#     'findingType': '1',\n",
    "# }\n",
    "\n",
    "# VRConfig['SET_SIZE'] = dataSetConfig['dataSetSize']\n",
    "\n",
    "# SGDConfig = VRConfig.copy()\n",
    "# SGDConfig['gamma'] = 2e-2\n",
    "\n",
    "# batchConfig = VRConfig.copy()\n",
    "# batchConfig['batchSize'] = 50\n",
    "# batchConfig['gamma'] = 1e-2\n",
    "\n",
    "# SVRGConfig = VRConfig.copy()\n",
    "# SVRGConfig['snapshotInterval'] = dataSetConfig['dataSetSize']\n",
    "# SVRGConfig['gamma'] = 2e-2\n",
    "\n",
    "# SAGAConfig = VRConfig.copy()\n",
    "# SAGAConfig['gamma'] = 2e-2\n",
    "\n",
    "# SARAHConfig = VRConfig.copy()\n",
    "# SARAHConfig['gamma'] = 2e-2\n",
    "\n",
    "# ByrD2SAGAConfig = VRConfig.copy()\n",
    "# ByrD2SAGAConfig['gamma'] = 2e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "covtype dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetConfig = {\n",
    "    'name': 'covtype',\n",
    "    'dataSet' : 'covtype.libsvm.binary.scale',\n",
    "    'dataSetSize': 581012,\n",
    "    'maxFeature': 54,\n",
    "    'findingType': '1',\n",
    "}\n",
    "\n",
    "VRConfig['SET_SIZE'] = dataSetConfig['dataSetSize']\n",
    "\n",
    "SGDConfig = VRConfig.copy()\n",
    "SGDConfig['gamma'] = 1e-2\n",
    "\n",
    "batchConfig = VRConfig.copy()\n",
    "batchConfig['batchSize'] = 50\n",
    "batchConfig['gamma'] = 5e-3\n",
    "\n",
    "SVRGConfig = VRConfig.copy()\n",
    "SVRGConfig['snapshotInterval'] = dataSetConfig['dataSetSize']\n",
    "SVRGConfig['gamma'] = 1e-2\n",
    "\n",
    "SAGAConfig = VRConfig.copy()\n",
    "SAGAConfig['gamma'] = 5e-3\n",
    "\n",
    "SARAHConfig = VRConfig.copy()\n",
    "SARAHConfig['gamma'] = 1e-2\n",
    "\n",
    "ByrD2SAGAConfig = VRConfig.copy()\n",
    "ByrD2SAGAConfig['gamma'] = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SET_SIZE = dataSetConfig['dataSetSize']\n",
    "maxFeature = dataSetConfig['maxFeature']\n",
    "findingType = dataSetConfig['findingType']\n",
    "\n",
    "CACHE_DIR = './cache/' + dataSetConfig['name'] + '_'\n",
    "# ====================================================\n",
    "# 报告函数\n",
    "def log(*k, **kw):\n",
    "    timeStamp = time.strftime('[%y-%m-%d %H:%M:%S] ', time.localtime())\n",
    "    print(timeStamp, end='')\n",
    "    print(*k, **kw)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def logAxis(path, Fmin):\n",
    "#     return [math.log10(p-Fmin) for p in path]\n",
    "    return [p-Fmin for p in path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L = np.sum([(scipy.sparse.linalg.norm(X[i, :]) + 1)\n",
    "#             ** 2 for i in range(X.shape[0])])\n",
    "# L = Lambda + 1/(4*SET_SIZE) * L\n",
    "\n",
    "torch.manual_seed(VRConfig['SEED'])#为CPU设置随机种子\n",
    "\n",
    "w0 = torch.zeros(maxFeature + 1, dtype=torch.float64)\n",
    "w0 = torch.nn.init.normal_(w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM_dataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, **dataSetConfig):\n",
    "        super(SVM_dataSet, self).__init__()\n",
    "        log('开始加载数据集')\n",
    "        self.X = torch.zeros((SET_SIZE, maxFeature), dtype=torch.float64)\n",
    "        self.Y = torch.zeros((SET_SIZE), dtype=torch.float64)\n",
    "        __dir__ = '.'\n",
    "        dataFile = __dir__ + '/dataset/' + dataSetConfig['dataSet']\n",
    "\n",
    "        with open(dataFile, 'r') as f:\n",
    "            posCount = 0\n",
    "            negCount = 1\n",
    "            for (line, vector) in enumerate(f):\n",
    "                (cat, data) = vector.split(' ', 1)\n",
    "                if cat == findingType:\n",
    "                    self.Y[line] = 1\n",
    "                    posCount += 1\n",
    "                else:\n",
    "                    self.Y[line] = 0\n",
    "                    negCount += 1\n",
    "                for piece in data.strip().split(' '):\n",
    "                    match = re.search(r'(\\S+):(\\S+)', piece)\n",
    "                    feature = int(match.group(1)) - 1  # 数据集从1开始\n",
    "                    value = float(match.group(2))\n",
    "                    # 插入矩阵\n",
    "                    self.X[line][feature] = value\n",
    "        log('加载数据集完成({})，正类：{}个，负类：{}个'.format(dataSetConfig['dataSet'], posCount, negCount))\n",
    "        \n",
    "        # 设置随机取样\n",
    "        self.__RR = False\n",
    "        self.__order = list(range(SET_SIZE))\n",
    "    def randomReshuffle(self):\n",
    "        self.__RR = True\n",
    "        random.shuffle(self.__order)\n",
    "    def resetShuffle(self):\n",
    "        self.RR = False\n",
    "    def __getitem__(self, index):\n",
    "        if self.__RR:\n",
    "            i = self.__order[index]\n",
    "            return self.X[i], self.Y[i]\n",
    "        else:\n",
    "            return self.X[index], self.Y[index]\n",
    "    def __len__(self):\n",
    "        return SET_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SVM_dataSet(**dataSetConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L = np.sum([(x.norm().item() + 1)** 2 for x, _ in dataset])\n",
    "# L = VRConfig['weight_decay'] + 1/(4*SET_SIZE) * L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def accuracy(w, dataset):\n",
    "    correct = 0\n",
    "    for data, label in dataset:\n",
    "        pre = LogisticRegression(w, data) > 0.5\n",
    "        correct += (pre.type(torch.uint8) == label.type(torch.uint8)).item()\n",
    "    return correct / len(dataset)\n",
    "def F(w, dataset, weight_decay):\n",
    "    loss = 0\n",
    "    for data, label in dataset:\n",
    "        predict = LogisticRegression(w, data)\n",
    "        loss += torch.nn.functional.binary_cross_entropy(predict, label)\n",
    "    loss /= len(dataset)\n",
    "    loss += weight_decay * torch.norm(w)**2 / 2\n",
    "    return loss.item()\n",
    "def G(w, dataset, weight_decay):\n",
    "    G = torch.zeros_like(w, requires_grad=False, dtype=torch.float64)\n",
    "    g = torch.zeros_like(w, requires_grad=False, dtype=torch.float64)\n",
    "    for index in range(len(dataset)):\n",
    "        x, y = dataset[index]\n",
    "        predict = LogisticRegression(w, x)\n",
    "\n",
    "        err = -(y-predict).data\n",
    "        g[:-1] = err*x\n",
    "        g[-1] = err\n",
    "        G.add_(1/len(dataset), g)\n",
    "    G.add_(weight_decay, w)\n",
    "    return G\n",
    "def LogisticRegression(w, x):\n",
    "    out = w[:-1].dot(x) + w[-1]\n",
    "    return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVarience(w_local, honestSize):\n",
    "    avg = w_local[:honestSize].mean(dim=0)\n",
    "    s = 0\n",
    "    for w in w_local[:honestSize]:\n",
    "        s += (w - avg).norm()**2\n",
    "    s /= honestSize\n",
    "    return s.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOuterVariation(w_min, honestSize):\n",
    "    # 数据分片\n",
    "    pieces = [(i*len(dataset)) // honestSize for i in range(honestSize+1)]\n",
    "    dataPerNode = [pieces[i+1] - pieces[i] for i in range(honestSize)]\n",
    "    \n",
    "    gradients = []\n",
    "    for node in range(honestSize):\n",
    "        gradient = torch.zeros_like(w_min)\n",
    "        for index in range(pieces[node], pieces[node+1]):\n",
    "            x, y = dataset[index]\n",
    "            # 更新梯度表\n",
    "            predict = LogisticRegression(w_min, x)\n",
    "\n",
    "            err = (predict-y).data\n",
    "            gradient[:-1].add_(err*x)\n",
    "            gradient[-1].add_(err)\n",
    "        gradient.div_(dataPerNode[node])\n",
    "        gradients.append(gradient)\n",
    "    gradients = torch.stack(gradients)\n",
    "    outerVariation = getVarience(gradients, honestSize)\n",
    "    return outerVariation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(wList):\n",
    "    return torch.mean(wList, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gm(wList):\n",
    "    max_iter = 80\n",
    "    tol = 1e-5\n",
    "    guess = torch.mean(wList, dim=0)\n",
    "    for _ in range(max_iter):\n",
    "        dist_li = torch.norm(wList-guess, dim=1)\n",
    "        for i in range(len(dist_li)):\n",
    "            if dist_li[i] == 0:\n",
    "                dist_li[i] = 1\n",
    "        temp1 = torch.sum(torch.stack([w/d for w, d in zip(wList, dist_li)]), dim=0)\n",
    "        temp2 = torch.sum(1/dist_li)\n",
    "        guess_next = temp1 / temp2\n",
    "        guess_movement = torch.norm(guess - guess_next)\n",
    "        guess = guess_next\n",
    "        if guess_movement <= tol:\n",
    "            break\n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gm7(wList):\n",
    "    max_iter = 80\n",
    "    tol = 1e-7\n",
    "    guess = torch.mean(wList, dim=0)\n",
    "    for _ in range(max_iter):\n",
    "        dist_li = torch.norm(wList-guess, dim=1)\n",
    "        for i in range(len(dist_li)):\n",
    "            if dist_li[i] == 0:\n",
    "                dist_li[i] = 1\n",
    "        temp1 = torch.sum(torch.stack([w/d for w, d in zip(wList, dist_li)]), dim=0)\n",
    "        temp2 = torch.sum(1/dist_li)\n",
    "        guess_next = temp1 / temp2\n",
    "        guess_movement = torch.norm(guess - guess_next)\n",
    "        guess = guess_next\n",
    "        if guess_movement <= tol:\n",
    "            break\n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Krum_(nodeSize, byzantineSize):\n",
    "    honestSize = nodeSize - byzantineSize\n",
    "    dist = torch.zeros(nodeSize, nodeSize, dtype=torch.float32)\n",
    "    def Krum(wList):\n",
    "        for i in range(nodeSize):\n",
    "            for j in range(i, nodeSize):\n",
    "                distance = wList[i].data - wList[j].data\n",
    "                distance = (distance*distance).sum()\n",
    "                distance = -distance # 两处都是取距离的最小值，需要改成负数\n",
    "                dist[i][j] = distance.data\n",
    "                dist[j][i] = distance.data\n",
    "        k = nodeSize - byzantineSize - 2 + 1 # 算上自己和自己的0.00\n",
    "        topv, _ = dist.topk(k=k, dim=1)\n",
    "        sumdist = topv.sum(dim=1)\n",
    "        resindex = sumdist.topk(1)[1].squeeze()\n",
    "        return wList[resindex]\n",
    "    return Krum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(wList):\n",
    "    return wList.median(dim=0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Quantized_l1sign(w):\n",
    "    # w: shape(NodeSize, parameter length)\n",
    "    \n",
    "    nodesize, paralength = w.size()\n",
    "    w_l1 = torch.norm(w, p=1, dim=1).view(nodesize, 1)\n",
    "    w_l1 = w_l1 / paralength\n",
    "    w = torch.sign(w) * w_l1\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Quantized_topk(w):\n",
    "    # w: shape(NodeSize, parameter length)\n",
    "    ratio = 0.1\n",
    "    \n",
    "    nodesize, paralength = w.size()\n",
    "    k = round(paralength * ratio)\n",
    "    _, index = torch.topk(abs(w), k, dim=1) #obtain the locations of topk absolute values\n",
    "    w_topk = torch.gather(w, 1, index) # extract the topk elements from w\n",
    "    #w_topk = paralength / k * w_topk\n",
    "    # obtain topk matrix of w with other elements being zero\n",
    "    w = torch.zeros(nodesize, paralength, dtype=torch.float64).scatter_(1, index, w_topk) \n",
    "    \n",
    "    #w = w/ratio\n",
    "    \n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Quantized_randk(w):\n",
    "    # w: shape(NodeSize, parameter length)\n",
    "    ratio = 0.1\n",
    "    \n",
    "    nodesize, paralength = w.size()\n",
    "    k = round(paralength * ratio)\n",
    "    index = torch.zeros(nodesize, paralength, dtype=torch.int64)\n",
    "    for i in range(0,nodesize):\n",
    "        index[i,:] = torch.randperm(paralength)\n",
    "    index = index[:,0:k]\n",
    "    w_randk = torch.gather(w, 1, index) # extract the randk elements from w\n",
    "    # obtain randk matrix of w with other elements being zero\n",
    "    w = torch.zeros(nodesize, paralength, dtype=torch.float64).scatter_(1, index, w_randk) \n",
    "    \n",
    "    w = w/ratio\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_threshold(w):\n",
    "    # w: shape(NodeSize, parameter length)\n",
    "    \n",
    "    nodesize, paralength = w.size()\n",
    "    beta = int(nodesize * 0.3)\n",
    "    w_l1 = torch.norm(w, p=1, dim=1)\n",
    "    \n",
    "    _, index = torch.sort(w_l1)\n",
    "    index_sele = index[:(nodesize - beta)]\n",
    "    w = torch.index_select(w, 0, index_sele)\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central SAGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def CentralSAGA(w0, gamma, weight_decay, rounds=10, displayInterval=1000, SEED=100, fixSeed=False, **kw):\n",
    "\n",
    "    # 初始化\n",
    "    w = w0.clone().detach()\n",
    "    \n",
    "    store = torch.zeros([SET_SIZE, w.size(0)], requires_grad=False, dtype=torch.float64)\n",
    "    for index in range(SET_SIZE):\n",
    "        x, y = dataset[index]\n",
    "        predict = LogisticRegression(w, x)\n",
    "\n",
    "        err = (predict-y).data\n",
    "        store[index][:-1] = err*x\n",
    "        store[index][-1] = err\n",
    "        store[index].add_(weight_decay, w)\n",
    "\n",
    "    G_avg = torch.mean(store, dim=0)\n",
    "    path = [F(w, dataset, weight_decay)]\n",
    "    \n",
    "    # 中间变量分配空间\n",
    "    new_G = torch.zeros(w0.size(), dtype=torch.float64)\n",
    "    \n",
    "    log('[SAGA]初始 loss={:.6f}, accuracy={:.2f} gamma={:}'.format(path[0], accuracy(w, dataset), gamma))\n",
    "    log('开始迭代')\n",
    "    for r in range(rounds):\n",
    "        for k in range(displayInterval):\n",
    "            # 更新梯度表\n",
    "            index = random.randint(0, SET_SIZE-1)\n",
    "\n",
    "            x, y = dataset[index]\n",
    "            predict = LogisticRegression(w, x)\n",
    "            \n",
    "            # 计算梯度\n",
    "            old_G = store[index]\n",
    "            err = (predict-y).data\n",
    "            new_G[:-1] = err*x\n",
    "            new_G[-1] = err\n",
    "            new_G.add_(weight_decay, w)\n",
    "            \n",
    "            gradient = new_G.data - old_G.data + G_avg.data\n",
    "            \n",
    "            G_avg.add_(1 / SET_SIZE, new_G.data - old_G.data)\n",
    "            store[index] = new_G.data\n",
    "            w.data.add_(-gamma, gradient.data)\n",
    "            \n",
    "        loss = F(w, dataset, weight_decay)\n",
    "        acc = accuracy(w, dataset)\n",
    "        path.append(loss)\n",
    "        log('[SAGA]已迭代 {}/{} rounds (interval: {:.0f}), loss={:.9f}, accuracy={:.2f}'.format(\n",
    "            r+1, rounds, displayInterval, loss, acc\n",
    "        ))\n",
    "    return w, path, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAGA_min(w0, dataset, gamma, weight_decay, epoch=1, **kw):\n",
    "\n",
    "    # 初始化\n",
    "    w = w0.clone().detach()\n",
    "    \n",
    "    store = torch.zeros([SET_SIZE, w.size(0)], requires_grad=False, dtype=torch.float64)\n",
    "    for index in range(SET_SIZE):\n",
    "        x, y = dataset[index]\n",
    "        predict = LogisticRegression(w, x)\n",
    "\n",
    "        err = -(y-predict).data\n",
    "        store[index][:-1] = err*x\n",
    "        store[index][-1] = err\n",
    "        store[index].add_(weight_decay, w)\n",
    "\n",
    "    G_avg = torch.mean(store, dim=0)\n",
    "    \n",
    "    # 中间变量分配空间\n",
    "    new_G = torch.zeros(w0.size(), dtype=torch.float64)\n",
    "    for e in range(epoch):\n",
    "        for _ in range(SET_SIZE):\n",
    "            # 更新梯度表\n",
    "            index = random.randint(0, SET_SIZE-1)\n",
    "\n",
    "            x, y = dataset[index]\n",
    "            predict = LogisticRegression(w, x)\n",
    "            \n",
    "            # 计算梯度\n",
    "            old_G = store[index]\n",
    "            err = -(y-predict).data\n",
    "            new_G[:-1] = err*x\n",
    "            new_G[-1] = err\n",
    "            new_G.add_(weight_decay, w)\n",
    "            \n",
    "            gradient = new_G.data - old_G.data + G_avg.data\n",
    "            \n",
    "            G_avg.add_(1 / SET_SIZE, new_G.data - old_G.data)\n",
    "            store[index] = new_G.data\n",
    "            w.data.add_(-gamma, gradient.data)\n",
    "        log('[SAGA]已迭代{:.0f}/{:.0f}趟'.format(e+1, epoch))\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     37
    ]
   },
   "outputs": [],
   "source": [
    "def SGD(w0, gamma, aggregate, weight_decay, honestSize=0, byzantineSize=0, attack=None,\n",
    "            rounds=10, displayInterval=1000, SEED=100, fixSeed=False, **kw):\n",
    "    assert byzantineSize == 0 or attack != None\n",
    "    assert honestSize != 0\n",
    "    \n",
    "    if fixSeed:\n",
    "        random.seed(SEED)\n",
    "\n",
    "    nodeSize = honestSize + byzantineSize\n",
    "    \n",
    "    # 初始化\n",
    "    w = w0.clone().detach()\n",
    "\n",
    "    # 数据分片\n",
    "    pieces = [(i*len(dataset)) // honestSize for i in range(honestSize+1)]\n",
    "    dataPerNode = [pieces[i+1] - pieces[i] for i in range(honestSize)]\n",
    "\n",
    "    path = [F(w, dataset, weight_decay)]\n",
    "    variencePath = []\n",
    "    log('[SGD]初始 loss={:.6f}, accuracy={:.2f} gamma={:}'.format(path[0], accuracy(w, dataset), gamma))\n",
    "    \n",
    "    # 中间变量分配空间\n",
    "    new_G = torch.zeros_like(w0, dtype=torch.float64)\n",
    "    message = torch.zeros(nodeSize, len(w0), dtype=torch.float64)\n",
    "    \n",
    "    quan = torch.zeros(nodeSize, len(w0), dtype=torch.float64)\n",
    "    mem = torch.zeros(nodeSize, len(w0), dtype=torch.float64)\n",
    "    quanc = torch.zeros(1, len(w0), dtype=torch.float64)\n",
    "    memc = torch.zeros(1, len(w0), dtype=torch.float64)\n",
    "    \n",
    "    message1 = torch.zeros(honestSize, len(w0), dtype=torch.float64)\n",
    "    message2 = torch.zeros(byzantineSize, len(w0), dtype=torch.float64)\n",
    "    H = torch.zeros(nodeSize, len(w0), dtype=torch.float64)\n",
    "    H1 = torch.ones(honestSize, len(w0), dtype=torch.float64)\n",
    "    H2 = torch.ones(byzantineSize, len(w0), dtype=torch.float64)\n",
    "    quan1 = torch.zeros(honestSize, len(w0), dtype=torch.float64)\n",
    "    quan2 = torch.zeros(byzantineSize, len(w0), dtype=torch.float64)\n",
    "    alpha = 1e-1\n",
    "\n",
    "    log('开始迭代')\n",
    "    for r in range(rounds):\n",
    "        for k in range(displayInterval):\n",
    "            # 诚实节点更新\n",
    "            for node in range(honestSize):\n",
    "                index = random.randint(pieces[node], pieces[node+1]-1)\n",
    "\n",
    "                x, y = dataset[index]\n",
    "                # 更新梯度表\n",
    "                predict = LogisticRegression(w, x)\n",
    "                err = (predict-y).data\n",
    "                new_G[:-1] = err*x\n",
    "                new_G[-1] = err\n",
    "                new_G.add_(weight_decay, w)\n",
    "                \n",
    "                gradient = new_G\n",
    "                \n",
    "                message[node].copy_(gradient.data)\n",
    "            \n",
    "            #Quantize the honest information  \n",
    "#             quan = Quantized_topk(message + mem) \n",
    "#             mem = message + mem - quan\n",
    "#             message = quan\n",
    "\n",
    "            # 同步\n",
    "            # Byzantine攻击\n",
    "            if attack != None:\n",
    "                attack(message, byzantineSize)\n",
    "            \n",
    "            \n",
    "              #Quantize all the information, double pass + error feedback\n",
    "#             quan = Quantized_topk(message + mem) \n",
    "#             mem = message + mem - quan\n",
    "#             message = quan https://live.bilibili.com/blackboard/activity-Sj6iU9MnS.html?visit_id=6x2jf6n3oxkw\n",
    "#             g = aggregate(message)\n",
    "#             quanc = Quantized_topk(g + memc)\n",
    "#             memc = g + memc - quanc\n",
    "#             g = quanc\n",
    "#             g = g.squeeze()\n",
    "            \n",
    "    \n",
    "            #Quantize all the information, single pass + error feedback\n",
    "#             quan = Quantized_l1sign(message + mem) \n",
    "#             mem = message + mem - quan\n",
    "#             message = quan          \n",
    "#             g = aggregate(message)\n",
    "\n",
    "             #Quantize all the information, single pass\n",
    "#             message1 = message[0:honestSize]\n",
    "#             message2 = message[honestSize:]\n",
    "#             quan1 = Quantized_randk(message1)\n",
    "#             quan2 = Quantized_topk(message2)\n",
    "#             message = torch.cat((quan1, quan2), 0)\n",
    "#             g = aggregate(message)\n",
    "\n",
    "            #Gradient Norm Threshold based method\n",
    "#             message1 = message[0:honestSize]\n",
    "#             message2 = message[honestSize:]\n",
    "#             quan1 = Quantized_randk(message1)\n",
    "#             quan2 = Quantized_topk(message2)\n",
    "#             message = torch.cat((quan1, quan2), 0)\n",
    "#             message_select = norm_threshold(message)          \n",
    "#             g = aggregate(message_select)\n",
    "            \n",
    "    \n",
    "            #SignSGD\n",
    "            message = torch.sign(message)\n",
    "            g = aggregate(message)\n",
    "        \n",
    "            #DIANA type Quantization, single pass\n",
    "#             message1 = message[0:honestSize]\n",
    "#             message2 = message[honestSize:]\n",
    "#             delta1 = message1 - H1\n",
    "#             quan1 = Quantized_randk(delta1)\n",
    "#             message1 = H1 + quan1\n",
    "#             delta2 = message2 - H2\n",
    "#             quan2 = Quantized_topk(delta2)\n",
    "#             message2 = H2 + quan2\n",
    "#             H1 = H1 + alpha * quan1\n",
    "#             H2 = H2 + alpha * quan2\n",
    "#             message = torch.cat((message1, message2), 0)\n",
    "#             g = aggregate(message)\n",
    "            \n",
    "#             delta = message - H\n",
    "#             quan = Quantized_randk(delta)\n",
    "#             message = H + quan\n",
    "#             H = H + alpha * quan\n",
    "#             g = aggregate(message)\n",
    "            \n",
    "\n",
    "#             g = aggregate(message)\n",
    "            w.add_(-gamma, g.data)\n",
    "        \n",
    "        loss = F(w, dataset, weight_decay)\n",
    "        acc = accuracy(w, dataset)\n",
    "        path.append(loss)\n",
    "        var = getVarience(message, honestSize)\n",
    "        variencePath.append(var)\n",
    "        log('[SGD]已迭代 {}/{} rounds (interval: {:.0f}), loss={:.9f}, accuracy={:.2f}, var={:.9f}'.format(\n",
    "            r+1, rounds, displayInterval, loss, acc, var\n",
    "        ))\n",
    "    return w, path, variencePath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     37
    ]
   },
   "outputs": [],
   "source": [
    "def BatchSGD(w0, gamma, aggregate, weight_decay, honestSize=0, byzantineSize=0, attack=None, batchSize=50,\n",
    "            rounds=10, displayInterval=1000, SEED=100, fixSeed=False, **kw):\n",
    "    assert byzantineSize == 0 or attack != None\n",
    "    assert honestSize != 0\n",
    "    \n",
    "    if fixSeed:\n",
    "        random.seed(SEED)\n",
    "\n",
    "    nodeSize = honestSize + byzantineSize\n",
    "    \n",
    "    # 初始化\n",
    "    w = w0.clone().detach()\n",
    "\n",
    "    # 数据分片\n",
    "    pieces = [(i*len(dataset)) // honestSize for i in range(honestSize+1)]\n",
    "    dataPerNode = [pieces[i+1] - pieces[i] for i in range(honestSize)]\n",
    "\n",
    "    path = [F(w, dataset, weight_decay)]\n",
    "    variencePath = []\n",
    "    log('[BatchSGD]初始 loss={:.6f}, accuracy={:.2f} gamma={:}'.format(path[0], accuracy(w, dataset), gamma))\n",
    "    \n",
    "    # 中间变量分配空间\n",
    "    new_G = torch.zeros_like(w0, dtype=torch.float64)\n",
    "    message = torch.zeros(nodeSize, len(w0), dtype=torch.float64)\n",
    "\n",
    "    log('开始迭代')\n",
    "    for r in range(rounds):\n",
    "        for k in range(displayInterval):\n",
    "            # 诚实节点更新\n",
    "            for node in range(honestSize):\n",
    "                gradient = torch.zeros_like(new_G)\n",
    "                for b in range(batchSize):\n",
    "                    index = random.randint(pieces[node], pieces[node+1]-1)\n",
    "\n",
    "                    x, y = dataset[index]\n",
    "                    # 更新梯度表\n",
    "                    predict = LogisticRegression(w, x)\n",
    "                    err = (predict-y).data\n",
    "                    new_G[:-1] = err*x\n",
    "                    new_G[-1] = err\n",
    "                    new_G.add_(weight_decay, w)\n",
    "                    gradient.add_(1/batchSize, new_G)\n",
    "                message[node].copy_(gradient.data)\n",
    "\n",
    "            # 同步\n",
    "            # Byzantine攻击\n",
    "            if attack != None:\n",
    "                attack(message, byzantineSize)\n",
    "            g = aggregate(message)\n",
    "            w.add_(-gamma, g.data)\n",
    "            \n",
    "        loss = F(w, dataset, weight_decay)\n",
    "        acc = accuracy(w, dataset)\n",
    "        path.append(loss)\n",
    "        var = getVarience(message, honestSize)\n",
    "        variencePath.append(var)\n",
    "        log('[BatchSGD]已迭代 {}/{} rounds (interval: {:.0f}), loss={:.9f}, accuracy={:.2f}, var={:.9f}'.format(\n",
    "            r+1, rounds, displayInterval, loss, acc, var\n",
    "        ))\n",
    "    return w, path, variencePath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     37
    ]
   },
   "outputs": [],
   "source": [
    "def SAGA(w0, gamma, aggregate, weight_decay, honestSize=0, byzantineSize=0, attack=None, \n",
    "            rounds=10, displayInterval=1000, SEED=100, fixSeed=False, **kw):\n",
    "    assert byzantineSize == 0 or attack != None\n",
    "    assert honestSize != 0\n",
    "    \n",
    "    if fixSeed:\n",
    "        random.seed(SEED)\n",
    "\n",
    "    nodeSize = honestSize + byzantineSize\n",
    "    \n",
    "    # 初始化\n",
    "    w = w0.clone().detach()\n",
    "\n",
    "    store = torch.zeros([len(dataset), w.size(0)], requires_grad=False, dtype=torch.float64)\n",
    "    for index in range(len(dataset)):\n",
    "        x, y = dataset[index]\n",
    "        predict = LogisticRegression(w, x)\n",
    "\n",
    "        err = (predict-y).data\n",
    "        store[index][:-1] = err*x\n",
    "        store[index][-1] = err\n",
    "        store[index].add_(weight_decay, w)\n",
    "\n",
    "    # 数据分片\n",
    "    pieces = [(i*len(dataset)) // honestSize for i in range(honestSize+1)]\n",
    "    dataPerNode = [pieces[i+1] - pieces[i] for i in range(honestSize)]\n",
    "\n",
    "    G_avg = torch.stack([\n",
    "        store[pieces[i]:pieces[i+1]].mean(dim=0) for i in range(honestSize)\n",
    "    ])\n",
    "    path = [F(w, dataset, weight_decay)]\n",
    "    variencePath = []\n",
    "    log('[SAGA]初始 loss={:.6f}, accuracy={:.2f} gamma={:}'.format(path[0], accuracy(w, dataset), gamma))\n",
    "    \n",
    "    # 中间变量分配空间\n",
    "    new_G = torch.zeros_like(w0, dtype=torch.float64)\n",
    "    message = torch.zeros(nodeSize, len(w0), dtype=torch.float64)\n",
    "    \n",
    "    quan = torch.zeros(nodeSize, len(w0), dtype=torch.float64)\n",
    "    mem = torch.zeros(nodeSize, len(w0), dtype=torch.float64)\n",
    "    quanc = torch.zeros(1, len(w0), dtype=torch.float64)\n",
    "    memc = torch.zeros(1, len(w0), dtype=torch.float64)\n",
    "    \n",
    "    message1 = torch.zeros(honestSize, len(w0), dtype=torch.float64)\n",
    "    message2 = torch.zeros(byzantineSize, len(w0), dtype=torch.float64)\n",
    "    H = torch.zeros(nodeSize, len(w0), dtype=torch.float64)\n",
    "    H1 = 0.01*torch.ones(honestSize, len(w0), dtype=torch.float64)\n",
    "    H2 = 0.01*torch.ones(byzantineSize, len(w0), dtype=torch.float64)\n",
    "    quan1 = torch.zeros(honestSize, len(w0), dtype=torch.float64)\n",
    "    quan2 = torch.zeros(byzantineSize, len(w0), dtype=torch.float64)\n",
    "    alpha = 0.001\n",
    "\n",
    "    log('开始迭代')\n",
    "    for r in range(rounds):\n",
    "        for k in range(displayInterval):\n",
    "            # 诚实节点更新\n",
    "            for node in range(honestSize):\n",
    "                index = random.randint(pieces[node], pieces[node+1]-1)\n",
    "\n",
    "                x, y = dataset[index]\n",
    "                # 更新梯度表\n",
    "                predict = LogisticRegression(w, x)\n",
    "\n",
    "                old_G = store[index]\n",
    "                err = (predict-y).data\n",
    "                new_G[:-1] = err*x\n",
    "                new_G[-1] = err\n",
    "                new_G.add_(weight_decay, w)\n",
    "\n",
    "                gradient = new_G.data - old_G.data + G_avg[node].data\n",
    "\n",
    "                G_avg[node].add_(1 / dataPerNode[node],\n",
    "                                 new_G.data - old_G.data)\n",
    "                store[index] = new_G.data\n",
    "\n",
    "                message[node].copy_(gradient.data)\n",
    "              \n",
    "            #Quantize the honest information  \n",
    "#             quan = Quantized_topk(message + mem) \n",
    "#             mem = message + mem - quan\n",
    "#             message = quan\n",
    "\n",
    "            # 同步\n",
    "            # Byzantine攻击\n",
    "            if attack != None:\n",
    "                attack(message, byzantineSize)\n",
    "     \n",
    "    \n",
    "            #Quantize all the information, double pass + error feedback\n",
    "#             quan = Quantized_topk(message + mem) \n",
    "#             mem = message + mem - quan\n",
    "#             message = quan\n",
    "#             g = aggregate(message)\n",
    "#             quanc = Quantized_topk(g + memc)\n",
    "#             memc = g + memc - quanc\n",
    "#             g = quanc\n",
    "#             g = g.squeeze()\n",
    "            \n",
    "    \n",
    "    \n",
    "             #Quantize all the information, single pass + error feedback\n",
    "#             quan = Quantized_l1sign(message + mem) \n",
    "#             mem = message + mem - quan\n",
    "#             message = quan          \n",
    "#             g = aggregate(message)\n",
    "\n",
    "\n",
    "             #Quantize all the information, single pass\n",
    "#             message1 = message[0:honestSize]\n",
    "#             message2 = message[honestSize:]\n",
    "#             quan1 = Quantized_randk(message1)\n",
    "#             quan2 = Quantized_topk(message2)\n",
    "#             message = torch.cat((quan1, quan2), 0)\n",
    "#             g = aggregate(message)\n",
    "\n",
    "\n",
    "            #DIANA type Quantization, single pass\n",
    "            message1 = message[0:honestSize]\n",
    "            message2 = message[honestSize:]\n",
    "            delta1 = message1 - H1\n",
    "            quan1 = Quantized_randk(delta1)\n",
    "            message1 = H1 + quan1\n",
    "            delta2 = message2 - H2\n",
    "            quan2 = Quantized_topk(delta2)\n",
    "            message2 = H2 + quan2\n",
    "            H1 = H1 + alpha * quan1\n",
    "            H2 = H2 + alpha * quan2\n",
    "            message = torch.cat((message1, message2), 0)\n",
    "            g = aggregate(message)\n",
    "            \n",
    "#             delta = message - H\n",
    "#             quan = Quantized_randk(delta)\n",
    "#             message = H + quan\n",
    "#             H = H + alpha * quan\n",
    "#             g = aggregate(message)\n",
    "            \n",
    "            #g = aggregate(message)\n",
    "            w.add_(-gamma, g.data)\n",
    "            \n",
    "        loss = F(w, dataset, weight_decay)\n",
    "        acc = accuracy(w, dataset)\n",
    "        path.append(loss)\n",
    "        var = getVarience(message, honestSize)\n",
    "        variencePath.append(var)\n",
    "        log('[SAGA]已迭代 {}/{} rounds (interval: {:.0f}), loss={:.9f}, accuracy={:.2f}, var={:.9f}'.format(\n",
    "            r+1, rounds, displayInterval, loss, acc, var\n",
    "        ))\n",
    "    return w, path, variencePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def white(messages, byzantinesize):\n",
    "    # 均值相同，方差较大\n",
    "    mu = torch.mean(messages[0:-byzantinesize], dim=0)\n",
    "    messages[-byzantinesize:].copy_(mu)\n",
    "    noise = torch.randn((byzantinesize, messages.size(1)), dtype=torch.float64)\n",
    "    messages[-byzantinesize:].add_(30, noise)\n",
    "def maxValue(messages, byzantinesize):\n",
    "    mu = torch.mean(messages[0:-byzantinesize], dim=0)\n",
    "    meliciousMessage = -3*mu\n",
    "    messages[-byzantinesize:].copy_(meliciousMessage)\n",
    "def zeroGradient(messages, byzantinesize):\n",
    "    s = torch.sum(messages[0:-byzantinesize], dim=0)\n",
    "    messages[-byzantinesize:].copy_(-s / byzantinesize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "def run(optimizer, aggregate, attack, config, recordInFile=True, markOnTitle='cd_randk0.001'):\n",
    "    \n",
    "    if attack == None:\n",
    "        title = '{}_{}_{}'.format(optimizer.__name__, 'baseline', aggregate.__name__)\n",
    "    else:\n",
    "        title = '{}_{}_{}'.format(optimizer.__name__, attack.__name__, aggregate.__name__)\n",
    "    if markOnTitle != '':\n",
    "        title = title + '_' + markOnTitle\n",
    "    print(dataSetConfig['name'] + '_' + title)\n",
    "    print('Fmin={}'.format(Fmin))\n",
    "\n",
    "    _config = config.copy()\n",
    "    _config['aggregate'] = aggregate\n",
    "    _config['attack'] = attack\n",
    "    attackName = 'baseline' if attack == None else attack.__name__\n",
    "    if attack == None:\n",
    "        _config['byzantineSize'] = 0\n",
    "        \n",
    "    # 打印运行信息\n",
    "    print('[提交任务] ' + dataSetConfig['name'] + '_' + title)\n",
    "    print('[运行信息]')\n",
    "    print('{:7s} name={} aggregation={} attack={}'.format('[优化方法]', optimizer.__name__, aggregate.__name__, attackName))\n",
    "    print('{:7s} gamma={} weight_decay={}'.format('[优化器设置]', _config['gamma'], _config['weight_decay']))\n",
    "    print('{:7s} honestSize={}, byzantineSize={}'.format('[节点个数]', _config['honestSize'], _config['byzantineSize']))\n",
    "    print('{:7s} rounds={}, displayInterval={}'.format('[运行次数]', _config['rounds'], _config['displayInterval']))\n",
    "    print('{:7s} SEED={}, fixSeed={}'.format('[torch设置]', _config['SEED'], _config['fixSeed']))\n",
    "    print('-------------------------------------------')\n",
    "    \n",
    "    log('提交任务')\n",
    "    try:\n",
    "        w, path, variancePath = optimizer(w0, **_config)\n",
    "\n",
    "        record = {\n",
    "            **dataSetConfig,\n",
    "            'gamma': _config['gamma'],\n",
    "            'weight_decay': _config['weight_decay'],\n",
    "            'honestSize': _config['honestSize'],\n",
    "            'byzantineSize': _config['byzantineSize'],\n",
    "            'rounds': _config['rounds'],\n",
    "            'displayInterval': _config['displayInterval'],\n",
    "            'path': path,\n",
    "            'variancePath': variancePath,\n",
    "        }\n",
    "\n",
    "        if recordInFile:\n",
    "            with open(CACHE_DIR + title, 'wb') as f:\n",
    "                pickle.dump(record, f)\n",
    "\n",
    "        axis = plt.axes()\n",
    "        plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "        axis.set_yscale('log')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正确性测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出现函数到达最小值后，重新回弹的现象，原因可能有\n",
    "1. 目标函数写错：忘记加惩罚项，忘记除以二等\n",
    "2. 触及机器精度边界"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = mean, attack = None, config = SAGAConfig, recordInFile = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从零开始跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_VRConfig = VRConfig.copy()\n",
    "_VRConfig['epoch'] = 10\n",
    "_VRConfig['gamma'] = 2e-2\n",
    "w_min = SAGA_min(w0, dataset, **_VRConfig)\n",
    "Fmin = F(w_min, dataset, _VRConfig['weight_decay'])\n",
    "print(Fmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "精度不够继续跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_VRConfig = VRConfig.copy()\n",
    "# _VRConfig['epoch'] = dataSetConfig['epoch'] * HONEST_SIZE\n",
    "_VRConfig['epoch'] = 20\n",
    "_VRConfig['gamma'] = 2e-2\n",
    "w_min = SAGA_min(w_min, dataset, **_VRConfig)\n",
    "Fmin = F(w_min, dataset, _VRConfig['weight_decay'])\n",
    "print(Fmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "存储Fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  with open(CACHE_DIR + 'Fmin', 'wb') as f:\n",
    "#      pickle.dump({\n",
    "#          'Fmin': Fmin,\n",
    "#          'w_min': w_min\n",
    "#      }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取Fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CACHE_DIR + 'Fmin', 'rb') as f:\n",
    "    obj = pickle.load(f)\n",
    "    Fmin, w_min = obj['Fmin'], obj['w_min']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = mean, attack = None, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = mean, attack = white, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = mean, attack = maxValue, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = mean, attack = zeroGradient, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - geomtric median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = gm, attack = None, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = gm, attack = white, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = gm, attack = maxValue, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = gm, attack = zeroGradient, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - Krum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=0)\n",
    "run(optimizer = SGD, aggregate = Krum, attack = None, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=VRConfig['byzantineSize'])\n",
    "run(optimizer = SGD, aggregate = Krum, attack = white, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=VRConfig['byzantineSize'])\n",
    "run(optimizer = SGD, aggregate = Krum, attack = maxValue, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Krum = Krum_(nodeSize=VRConfig['honestSize'], byzantineSize=VRConfig['byzantineSize'])\n",
    "run(optimizer = SGD, aggregate = Krum, attack = zeroGradient, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD - Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = median, attack = None, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = median, attack = white, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = median, attack = maxValue, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(optimizer = SGD, aggregate = median, attack = zeroGradient, config = SGDConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchSGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchSGD - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = batchConfig.copy()\n",
    "_VRConfig['aggregate'] = aggregate_linear\n",
    "_VRConfig['attack'] = None\n",
    "_VRConfig['byzantineSize'] = 0\n",
    "\n",
    "w, path, variancePath = FedBatchSGD(w0, **_VRConfig)\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'batchSize': _VRConfig['batchSize'],\n",
    "    'path': path,\n",
    "    'variancePath': variancePath,\n",
    "}\n",
    "\n",
    "with open(CACHE_DIR + 'BatchSGD_baseline_mean', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = batchConfig.copy()\n",
    "_VRConfig['aggregate'] = aggregate_linear\n",
    "_VRConfig['attack'] = whiteNoise\n",
    "w, path, variancePath = FedBatchSGD(w0, **_VRConfig)\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'batchSize': _VRConfig['batchSize'],\n",
    "    'path': path,\n",
    "    'variancePath': variancePath,\n",
    "}\n",
    "\n",
    "with open(CACHE_DIR + 'BatchSGD_white_mean', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = batchConfig.copy()\n",
    "_VRConfig['aggregate'] = aggregate_linear\n",
    "_VRConfig['attack'] = maxValue\n",
    "w, path, variancePath = FedBatchSGD(w0, **_VRConfig)\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'batchSize': _VRConfig['batchSize'],\n",
    "    'path': path,\n",
    "    'variancePath': variancePath,\n",
    "}\n",
    "\n",
    "with open(CACHE_DIR + 'BatchSGD_maxValue_mean', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = batchConfig.copy()\n",
    "_VRConfig['aggregate'] = aggregate_linear\n",
    "_VRConfig['attack'] = zeroGradient\n",
    "w, path, variancePath = FedBatchSGD(w0, **_VRConfig)\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'batchSize': _VRConfig['batchSize'],\n",
    "    'path': path,\n",
    "    'variancePath': variancePath,\n",
    "}\n",
    "\n",
    "with open(CACHE_DIR + 'BatchSGD_zeroGradient_mean', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchSGD - geomtric median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = batchConfig.copy()\n",
    "_VRConfig['aggregate'] = aggregate_geometric\n",
    "_VRConfig['attack'] = None\n",
    "_VRConfig['byzantineSize'] = 0\n",
    "w, path, variancePath = FedBatchSGD(w0, **_VRConfig)\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'batchSize': _VRConfig['batchSize'],\n",
    "    'path': path,\n",
    "    'variancePath': variancePath,\n",
    "}\n",
    "\n",
    "with open(CACHE_DIR + 'BatchSGD_baseline_gm', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = batchConfig.copy()\n",
    "_VRConfig['aggregate'] = aggregate_geometric\n",
    "_VRConfig['attack'] = whiteNoise\n",
    "w, path, variancePath = FedBatchSGD(w0, **_VRConfig)\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'batchSize': _VRConfig['batchSize'],\n",
    "    'path': path,\n",
    "    'variancePath': variancePath,\n",
    "}\n",
    "\n",
    "with open(CACHE_DIR + 'BatchSGD_white_gm', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = batchConfig.copy()\n",
    "_VRConfig['aggregate'] = aggregate_geometric\n",
    "_VRConfig['attack'] = maxValue\n",
    "w, path, variancePath = FedBatchSGD(w0, **_VRConfig)\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'batchSize': _VRConfig['batchSize'],\n",
    "    'path': path,\n",
    "    'variancePath': variancePath,\n",
    "}\n",
    "\n",
    "with open(CACHE_DIR + 'BatchSGD_maxValue_gm', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log('Fmin={}'.format(Fmin))\n",
    "\n",
    "_VRConfig = batchConfig.copy()\n",
    "_VRConfig['aggregate'] = aggregate_geometric\n",
    "_VRConfig['attack'] = zeroGradient\n",
    "w, path, variancePath = FedBatchSGD(w0, **_VRConfig)\n",
    "\n",
    "record = {\n",
    "    **dataSetConfig,\n",
    "    'gamma': _VRConfig['gamma'],\n",
    "    'batchSize': _VRConfig['batchSize'],\n",
    "    'path': path,\n",
    "    'variancePath': variancePath,\n",
    "}\n",
    "\n",
    "with open(CACHE_DIR + 'BatchSGD_zeroGradient_gm', 'wb') as f:\n",
    "    pickle.dump(record, f)\n",
    "    \n",
    "axis = plt.axes()\n",
    "plt.plot(list(range(len(path))), logAxis(path, Fmin))\n",
    "axis.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAGA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAGA - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = mean, attack = None, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = mean, attack = white, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = mean, attack = maxValue, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = mean, attack = zeroGradient, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAGA - geomtric median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = gm, attack = None, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = gm, attack = white, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = gm, attack = maxValue, config = SAGAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(optimizer = SAGA, aggregate = gm, attack = zeroGradient, config = SAGAConfig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "693px",
    "left": "1481px",
    "right": "20px",
    "top": "125px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
